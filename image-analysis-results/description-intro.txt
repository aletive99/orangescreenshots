You are an expert in explaining Orange data mining workflows.

Following are three Orange data mining workflows. For each workflow a target description is given, so you can learn how we would like the workflows to be described. Orange components (widgets) are the working units of the program and can be connected with each other to perform different operations according to the needs of the user.
Then the information about the workflows’ connections are given and you will use them when the description is not given to understand and describe what the workflow does. The workflow information is given with the following format:
- Information about the widgets: in each line the widget name and how many times it appears in the workflow, separated by “/“
- Information about the widget connections: in each line, there is a sending component and a receiving component, separated with “->“. If there are multiple instances of the same widget, there will be information about which instance of the widget is being considered within parentheses. e.g. Data Table (2) means that we are referring to the second Data Table widget
- Short description of the function of each widget and their possible inputs and outputs.


ds-regression/030-reg-and-accuracy/workflow.png:
This workflow begins with the Paint Data widget, where users manually create a dataset by painting data points on a 2D plane, allowing for customized data generation. The painted dataset is then processed through the Select Columns widget, which enables users to manually select and configure the data attributes needed for further analysis. The selected data proceeds to the Data Sampler, where a subset of instances is chosen for analysis, maintaining a portion of the data as out-of-sample for testing purposes. The sampled data is utilized in a Polynomial Regression model to create a predictive model. The Linear Regression serves as a base learner for the Polynomial Regression, which is specifically tailored to demonstrate the impact of using different polynomial regressors on the data. Finally, the Test and Score widget evaluates the performance of the created models using both sampled and out-of-sample data, providing a comprehensive evaluation of the models' accuracy and effectiveness in predicting outcomes based on the input data. This workflow integrates data creation, selection, sampling, modeling, and testing into a seamless process, facilitating educational insights into the application of regression analysis in data science.

Paint Data/1
Select Columns/1
Data sampler/1
Linear Regression/1
Polynomial Regression/1
Test and Score/1

Paint Data -> Select Columns
Select Columns -> Data Sampler
Data Sampler -> Test and Score
Data Sampler -> Test and Score
Data Sampler -> Polynomial Regression
Linear Regression -> Polynomial Regression
Polynomial Regression -> Test and Score


Paint Data: Paints data on a 2D plane. You can place individual data points or use a brush to paint larger datasets.
Outputs
Data: dataset as painted in the plot

Select Columns: Manual selection of data attributes and composition of data domain.
Inputs
Data: input dataset
Outputs
Data: dataset with columns as set in the widget

Data Sampler: Selects a subset of data instances from an input dataset.
Inputs
Data: input dataset
Outputs
Data Sample: sampled data instances
Remaining Data: out-of-sample data

Linear Regression: A linear regression algorithm with optional L1 (LASSO), L2 (ridge) or L1L2 (elastic net) regularization.
Inputs
Data: input dataset
Preprocessor: preprocessing method(s)
Outputs
Learner: linear regression learning algorithm
Model: trained model
Coefficients: linear regression coefficients

Polynomial Regression: Educational widget that interactively shows regression line for different regressors.
Inputs
Data: input data set. It needs at least two continuous attributes.
Preprocessor: data preprocessors
Learner: regression algorithm used in the widget. Default set to Linear Regression.
Outputs
Learner: regression algorithm used in the widget
Predictor: trained regressor
Coefficients: regressor coefficients if any

Test and Score: Tests learning algorithms on data. 
Inputs
Data: input dataset
Test Data: separate data for testing
Learner: learning algorithm(s)
Outputs
Evaluation Results: results of testing classification algorithms



text-mining/050-topic-modeling/workflow.png:
The workflow begins with the Corpus widget, which loads a collection of text documents that can optionally be categorized, providing the initial dataset. This corpus is then fed into the Preprocess Text widget, which can apply various preprocessing techniques like tokenization, stemming, and stop words removal to prepare the text for further analysis. Next, the processed text is converted into a Bag of Words format through the corresponding widget, creating a token frequency representation for numerical analysis. The resultant corpus then undergoes topic modeling, using algorithms such as Latent Dirichlet Allocation (LDA), Latent Semantic Indexing (LSI), or Hierarchical Dirichlet Process (HDP) to identify and weigh topics within the documents. For visualization and deeper analysis, the identified topics are explored through the LDAvis widget, offering an interactive visual exploration of the topics, and the MDS (Multidimensional Scaling) widget, which visually projects topic similarities on a plane based on semantic distances, thus aiding in understanding both the individual topic distribution and their broader relationships within the corpus.

Corpus/1
Preprocess Text/1
Bag of Words/1
Topic Modelling/1
LDAvis/1
MDS/1

Corpus -> Preprocess Text
Preprocess Text -> Bag of Words
Bag of Words -> Topic Modelling 
Topic Modelling -> LDAvis
Topic Modelling -> MDS


Corpus: Load a corpus of text documents, (optionally) tagged with categories, or change the data input signal to the corpus.
Inputs
Data: Input data (optional)
Outputs
Corpus: A collection of documents.

Preprocess Text: Preprocesses corpus with selected methods.
Inputs
Corpus: A collection of documents.
Outputs
Corpus: Preprocessed corpus.

Bag of Words: Generates a bag of words from the input corpus.
Inputs
Corpus: A collection of documents.
Outputs
Corpus: Corpus with bag of words features appended.

Topic Modelling: Topic modelling with Latent Dirichlet Allocation, Latent Semantic Indexing or Hierarchical Dirichlet Process.
Inputs
Corpus: A collection of documents.
Outputs
Corpus: Corpus with topic weights appended.
Topics: Selected topics with word weights.
All Topics: Token weights per topic.

LDAvis: Interactive exploration of LDA topics.
Inputs
Topics: All LDA topics from topic modeling.

MDS: Multidimensional scaling (MDS) projects items onto a plane fitted to given distances between points.
Inputs
Data: input dataset
Distances: distance matrix
Data Subset: subset of instances
Outputs
Selected Data: instances selected from the plot
Data: dataset with MDS coordinates



ds-classification/220-fss-random/cv-on-fss.png
This workflow loads the datasets through the File widget. The data is then passed to the Randomize widget that randomizes the correspondence of classes and features in the dataset. The obtained modified dataset is then given to the Preprocess widget where the user can choose a method to preprocess the data according to their needs. The preprocessed dataset is then passed down to the Test and Score widget which uses the learners passed down by the Logistic Regression and the Random Forest widgets to predict the class from the data and then shows the results. This workflow ensures that the models are not only tailored to the characteristics of the randomized data but also rigorously evaluated to provide reliable insights.

File/1
Randomize/1
Preprocess/1
Logistic Regression/1
Random Forest/1
Test and Score/1

File -> Randomize
Randomize -> Preprocess
Preprocess -> Test and Score
Logistic Regression -> Test and Score
Random Forest -> Test and Score


File: Reads attribute-value data from an input file.
Outputs
Data: dataset from the file

Randomize: Shuffles classes, attributes and/or metas of an input dataset.
Inputs
Data: input dataset
Outputs
Data: randomized dataset

Preprocess: Preprocesses data with selected methods.
Inputs
Data: input dataset
Outputs
Preprocessor: preprocessing method
Preprocessed Data: data preprocessed with selected methods

Logistic Regression:The logistic regression classification algorithm with LASSO (L1) or ridge (L2) regularization.
Inputs
Data: input dataset
Preprocessor: preprocessing method(s)
Outputs
Learner: logistic regression learning algorithm
Model: trained model
Coefficients: logistic regression coefficients

Random Forest: Predict using an ensemble of decision trees.
Inputs
Data: input dataset
Preprocessor: preprocessing method(s)
Outputs
Learner: random forest learning algorithm
Model: trained model

Test and Score: Tests learning algorithms on data.
Inputs
Data: input dataset
Test Data: separate data for testing
Learner: learning algorithm(s)
Outputs
Evaluation Results: results of testing classification algorithms



——————————————————
Given the information provided so far and the knowledge you have about this topics, describe in a single paragraph similar to the examples provided the following workflow:

