comparative-model-evaluation.png:
  concise:
    "The workflow considers a classification dataset and compares the cross-validated performance of the three different classifiers: logistic regression, random forest and SVM. Cross-validated results are then checked in a confusion matrix. Additionally a data table is used to check the input dataset, and another data table is used to list the data instances where the classifiers were correct, or wrong, depending on the choice made in the confusion matrix."
  detailed:
    "This workflow first loads a dataset from a local file using the File widget. The dataset is then fed into the Test & Score widget along with three different machine learning models: Logistic Regression, Support Vector Machine (SVM), and Random Forest. The Test & Score widget evaluates these models using cross validation techniques and outputs performance metrics, which are passed down to the Confusion Matrix widget, which displays the proportions of correctly and incorrectly classified instances. The Confusion Matrix allows for manual selection of specific data instances, which are visualized in a Data Table for further examination. Additionally, the initial dataset is also visualized in a separate Data Table. This workflow allows the user to compare the performance of multiple classification models, identify misclassified instances, and explore the dataset in detail."

create-instance-logistic-regression-analysis.png:
  concise:
    "This workflow considers a classification dataset which is used to train a logistic regression model. Data instances are manually created and added to the original dataset to help the user see how the model classifies the data. This is allowed through the visualization of the predictions and the nomogram, which provides insights into the effects of attributes on class selection."
  detailed:
    "This workflow begins by loading a dataset from a local file using the File widget. The dataset is then used in two parallel paths. In the first path, the dataset is used to train a Logistic Regression model. The trained model is then passed to the Predictions widget, which also receives a new instance created by the Create Instance widget. This widget allows the user to manually set values for the instance based on the input dataset. The Predictions widget outputs the dataset with added predictions. In the second path, the Logistic Regression model and the created instance are both sent to the Nomogram widget, which provides a visual representation of the classifier and the effects of attributes on class probabilities. This workflow allows the user to create a new instance, predict its class using a logistic regression model, and visualize the model's structure and the influence of different attributes on the predictions."

decision-tree-data-visualization.png:
  concise:
    "This workflow uses the imported dataset to train a tree. The resulting model is then visualized in the Tree Viewer widget, allowing the user to select data instances from different branches. The subset created this way is then visualized in a scatter plot along with the full original dataset and in a box plot, providing insights on potential relationships between features and classes."
  detailed:
    "This workflow begins by loading a dataset from a local file using the File widget. The dataset is then passed to the Tree widget, which builds a decision tree model based on the input data. The resulting tree model is sent to the Tree Viewer widget, which provides a 2D visualization of the decision tree, allowing users to explore different nodes and their associated data. Selected data from the Tree Viewer can then be visualized in a Box Plot, showing the distribution of attribute values, and in a Scatter Plot, providing a 2D visualization of the selected instances. Additionally, the entire dataset from the File widget is directly visualized in the Scatter Plot to compare with the selected data from the tree nodes. This workflow enables the user to build, visualize, and explore a decision tree model, providing insights into the structure and characteristics of the dataset."

hierarchical-clustering-analysis.png:
  concise:
    "This clustering workflow loads a dataset from an online repository and allows the user to select the columns to be considered to cluster the data. The chosen features are used to create a dendrogram based on the distance between the data instances. The user is then allowed to select different clusters of data to be visualized in a scatter plot, enabling them to find potential relationships between clusters and specific features."
  detailed:
    "This workflow begins by loading a dataset from an online repository using the Datasets widget. The dataset is then passed to the Select Columns widget, which allows the user to manually choose which attributes to include in the analysis. The selected columns are then used to compute distances between data points using the Distances widget. These distances are subsequently used by the Hierarchical Clustering widget to perform hierarchical clustering and visualize the resulting dendrogram. Finally, the clustered data is visualized in a Scatter Plot, allowing the user to explore the relationships and patterns within the data. This workflow enables the user to load a dataset, select relevant features, perform hierarchical clustering, and visualize the clusters to gain insights into the data structure."

image-clustering-analysis.png:
  concise:
    "This data analysis pipeline reads a data file containing links to photos, and enables the user to cluster the photos. To enable clustering, the photos are embedded in the vector space (Image Embedding); these numeric charactecterizations of photos are then used to assess the distances (similarities) between photos. The input data file reports on the geo locations of places where the photos were taken, and user either displays all of these locations in the geo map, or locations of photos from selected cluster. The geo map used to visualize the selected cluster from the dendrogram allows the user to select data instances to be visualized both in an image and in a tabular format."
  detailed:
    "This workflow starts by loading a dataset from a local file using the File widget. The dataset, which includes images, is then processed by the Image Embedding widget to generate feature vectors for each image using deep learning models. These embeddings are passed to the Distances widget to compute a distance matrix, which is subsequently used by the Hierarchical Clustering widget to perform clustering and generate a dendrogram. The clusters are visualized on a Geo Map, which plots the data points based on their geographical coordinates. The Geo Map allows for the selection of data points, which are then displayed in a Data Table and an Image Viewer for further inspection. Additionally, the original dataset is also visualized on a separate Geo Map. This workflow enables the user to analyze image data by embedding it into a feature space, performing hierarchical clustering, and visualizing the results on a geographical map, facilitating the exploration of spatial patterns and cluster characteristics."

image-embedding-visualization.png:
  concise:
    "In this workflow, different images are loaded from a selected directory and turned into a dataset. The found images are both shown in the Image Viewer widget and in a tabular format highlighting their main characteristics. The images are also embedded into a feature space, which is then visualized in a data table."
  detailed:
    "This workflow begins by importing images from a directory using the Import Images widget. The imported images are then processed in three different ways: they are passed to the Image Embedding widget to generate feature vectors for each image, visualized through the Image Viewer widget, and displayed in a Data Table. The Image Embedding widget outputs the enhanced data table with image descriptors, which is then visualized in another Data Table. This workflow allows the user to import, visualize, and analyze images by embedding them into a feature space, facilitating further exploration and analysis of image data."

impute-rank-scatter-analysis.png:
  concise:
    "In the workflow, the user selects the features that are most correlated with a class attribute and passes the dimensionally reduced dataset to the scatter plot. To display all the data instances in the scatter plot, the workflow imputes the unknown values before the start of analysis. Feature selection helps the user in being able to, for a scatter plot visualization, choose only among a few, most important features."
  detailed:
    "This workflow begins by loading a dataset from a local file using the File widget. The loaded data is then passed to the Impute widget, which handles any missing values in the dataset by substituting them with computed or user-defined values. The imputed data is subsequently sent to the Rank widget, where variables are scored based on their correlation with a target variable or through applicable scorers. Finally, the scored data is visualized using the Scatter Plot widget, allowing the user to explore and analyze the relationships between different features in a two-dimensional plot. This workflow facilitates data preprocessing, feature ranking, and visual analysis, helping users understand key attributes and their interactions within the dataset."

k-means-analysis-and-visualization.png:
  concise:
    "This workflow allows the user to manually draw a dataset of their choosing. The data is then clustered with the k-means alogrithm. From this analysis both a silhouette plot and a scatter plot are created, respectively showing the silhouette scores and the distribution of data points of both the original dataset and selected data from the Silhouette Plot widget."
  detailed:
    "This workflow begins by creating a new dataset through the Paint Data widget, where users can manually place data points on a two-dimensional plane. The generated dataset is then passed to the k-Means widget, which applies the k-Means clustering algorithm and outputs a dataset with cluster labels. The clustering results are visualized using the Silhouette Plot, which assesses the quality of the clusters by showing the silhouette scores. Finally, the data, including cluster labels and silhouette scores, as well as selected data from the Silhouette Plot widget is visualized in a Scatter Plot, allowing the user to explore the clustering results and the distribution of data points. This workflow enables the user to manually create a dataset, perform k-Means clustering, evaluate cluster quality, and visualize the results."

preprocessed-text-word-cloud-analysis.png:
  concise:
    "In this workflow, a corpus is loaded and immediately visualized to allow for spontaneus exploration. It is then preprocessed using user selected techniques, for example tokenization, filtering, etc. Finally, the words that appear more frequently in the resulting corpus are shown through a word cloud."
  detailed:
    "This workflow first loads a collection of text documents through the Corpus widget. The loaded corpus is then preprocessed using the Preprocess Text widget, which applies various text processing techniques such as tokenization, filtering, normalization, and tagging. The preprocessed corpus is then passed to the Word Cloud widget, which visualizes the frequency of words in a word cloud, allowing users to select words for further analysis. Additionally, the original corpus is also viewed using the Corpus Viewer widget, enabling users to inspect the text documents directly. This workflow allows users to load, preprocess, and visualize text data, facilitating the exploration and analysis of word frequencies and document contents."

survival-data-analysis.png:
  concise:
    "This workflows loads a dataset that is immediately visualized in a tabular format and turned into survival data. The resulting dataset is visualized in a Kaplan-Meier plot which provides a visual representation of the estimated survival function."
  detailed:
    "This workflow begins by loading a dataset from an online repository using the Datasets widget. The dataset is then passed to the As Survival Data widget, where the user can manually select which features will be treated as the Time and Event target variables for survival analysis. The resulting survival dataset is then visualized using the Kaplan-Meier Plot widget, which provides a visual representation of the estimated survival function, showing the probability of an event at respective time intervals. Users can interactively select data instances from the Kaplan-Meier plot for further analysis. Additionally, the original dataset is also visualized in a Data Table for manual inspection and selection of data instances. This workflow allows the user to perform survival analysis on a dataset, visualize survival functions, and interactively explore the data."